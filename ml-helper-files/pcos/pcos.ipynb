{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1611832338293
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from train import clean_data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We start by setting up our experiment in our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1611832340213
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: quick-starts-ws-136366\n",
      "Azure region: southcentralus\n",
      "Subscription id: 5a4ab2ba-6c51-4805-8155-58759ad589d8\n",
      "Resource group: aml-quickstarts-136366\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'pcos_automl'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset for PCOS is a real-time data set that taken from a survey conducted among 119 women between the\n",
    "ages of 18 and 22. The dataset is primarily based on their lifestyle and food intake habits. The symptoms i.e.\n",
    "attributes are classified based on classification algorithms to predict whether the patient may have PCOS or not. The\n",
    "database consists of 119 samples with 18 attributes belonging to two different classes (maybe or maybe not).\n",
    "There are 14 binary attributes and 4 categorical attributes. <br/>\n",
    "PCOS-Survey/PCOSData. (2017). GitHub. Retrieved 30\n",
    "November 2017, from https://github.com/PCOSSurvey/PCOSData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832344899
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ds = TabularDatasetFactory.from_delimited_files(path=\"https://raw.githubusercontent.com/priyanshisharma/AI-Champ/master/pcos_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Here, we clean and observe our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1611832345447
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>regular_periods</th>\n",
       "      <th>rapid_weight_gain</th>\n",
       "      <th>excess_hair</th>\n",
       "      <th>dark_patches</th>\n",
       "      <th>pimples</th>\n",
       "      <th>depression_and_anxiety</th>\n",
       "      <th>diabetic_hypertension_family_history</th>\n",
       "      <th>body_weight_maintain_difficulty</th>\n",
       "      <th>oily_skin</th>\n",
       "      <th>...</th>\n",
       "      <th>eat_frequency</th>\n",
       "      <th>regular_excercise</th>\n",
       "      <th>sleep_time</th>\n",
       "      <th>wake_time</th>\n",
       "      <th>hostel_stress</th>\n",
       "      <th>personal_stress</th>\n",
       "      <th>peer_pressure</th>\n",
       "      <th>dietary_stress</th>\n",
       "      <th>fast_food_frequency</th>\n",
       "      <th>PCOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hb</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>hm</td>\n",
       "      <td>n</td>\n",
       "      <td>11.00pm</td>\n",
       "      <td>7.00am</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>w</td>\n",
       "      <td>mb n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>im</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>hm</td>\n",
       "      <td>n</td>\n",
       "      <td>12.00am</td>\n",
       "      <td>8.00am</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hb</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>hm</td>\n",
       "      <td>n</td>\n",
       "      <td>11.00pm</td>\n",
       "      <td>7.00am</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>w</td>\n",
       "      <td>mb n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ib</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>hm</td>\n",
       "      <td>n</td>\n",
       "      <td>3.00am</td>\n",
       "      <td>7.30am</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>mb n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>im</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>hm</td>\n",
       "      <td>n</td>\n",
       "      <td>12.00am</td>\n",
       "      <td>8.30am</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>w</td>\n",
       "      <td>mb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column1 regular_periods rapid_weight_gain excess_hair dark_patches pimples  \\\n",
       "0        0              hb                 y           y            n       y   \n",
       "1        1              im                 y           y            n       n   \n",
       "2        2              hb                 y           y            n       n   \n",
       "3        3              ib                 n           y            n       n   \n",
       "4        4              im                 y           y            n       n   \n",
       "\n",
       "  depression_and_anxiety diabetic_hypertension_family_history  \\\n",
       "0                      y                                    n   \n",
       "1                      y                                    n   \n",
       "2                      y                                    n   \n",
       "3                      y                                    y   \n",
       "4                      y                                    y   \n",
       "\n",
       "  body_weight_maintain_difficulty oily_skin  ... eat_frequency  \\\n",
       "0                               y         y  ...            hm   \n",
       "1                               y         n  ...            hm   \n",
       "2                               y         n  ...            hm   \n",
       "3                               y         n  ...            hm   \n",
       "4                               y         y  ...            hm   \n",
       "\n",
       "  regular_excercise sleep_time wake_time hostel_stress personal_stress  \\\n",
       "0                 n    11.00pm    7.00am             n               y   \n",
       "1                 n    12.00am    8.00am             n               n   \n",
       "2                 n    11.00pm    7.00am             n               n   \n",
       "3                 n     3.00am    7.30am             n               n   \n",
       "4                 n    12.00am    8.30am             n               y   \n",
       "\n",
       "  peer_pressure dietary_stress fast_food_frequency  PCOS  \n",
       "0             n              n                   w  mb n  \n",
       "1             n              y                   w    mb  \n",
       "2             y              n                   w  mb n  \n",
       "3             n              y                   w  mb n  \n",
       "4             n              n                   w    mb  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = clean_data(ds)\n",
    "\n",
    "df = pd.concat([x, pd.DataFrame(y)], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we split our data in order to fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832345708
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832345795
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "outname2='training_dataset3.csv'\n",
    "outdir2='training3/'\n",
    "if not os.path.exists(outdir2):\n",
    "    os.mkdir(outdir2)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.8)\n",
    "\n",
    "fullpath2=os.path.join(outdir2,outname2)\n",
    "df_test.to_csv(fullpath2)\n",
    "\n",
    "outname='validation_dataset3.csv'\n",
    "outdir='validation3/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "fullpath=os.path.join(outdir,outname)\n",
    "df_test.to_csv(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832345933
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we store our dataset in our default datastore in order to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832346046
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1611832346157
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Target already exists. Skipping upload for data/training_dataset3.csv\n",
      "Uploaded 0 files\n",
      "Uploading an estimated of 1 files\n",
      "Target already exists. Skipping upload for data/validation_dataset3.csv\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_a0fecb791d2f414b85d23918b92bd3f8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore.upload(src_dir = \"training3/\", target_path = \"data/\")\n",
    "datastore.upload(src_dir = \"validation3/\", target_path = \"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832347523
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "training_data = TabularDatasetFactory.from_delimited_files(path = [(datastore, (\"data/training_dataset3.csv\"))])\n",
    "validation_data = TabularDatasetFactory.from_delimited_files(path = [(datastore, (\"data/validation_dataset3.csv\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832347731
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data.to_pandas_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "We start by setting up our compute cluster, where we will run our automl run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832348180
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "cpu_cluster_name = \"cpucluster-aml\"\n",
    "\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We've used the following configuration for our run:\n",
    "\n",
    "|Setting |Reasons|\n",
    "|-|-|\n",
    "|**experiment_timeout_minutes**| Maximum amount of time in minutes that all iterations combined can take before the experiment terminates. I've taken this to be 30 mins due to the presence of 730 rows. |\n",
    "|**max_concurrent_iterations**|These are the iterations occuring simultaneously and has to be equal to the number of nodes in the cluster(5-1))|\n",
    "|**n_cross_validations**|Using 5 cross validations to avoi8d overfitting) |\n",
    "|**primary_metric**|Since the data isn't quite balanced, Weighted Average Precision Score |\n",
    "|**task**|Classification |\n",
    "|**compute_target**|This is the compute cluster we will be using |\n",
    "|**training_data**|This is the training dataset stored in the default datastore  |\n",
    "|**label_column_name**|This is the target variable|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611832348267
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'data/training_dataset3.csv')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\",\n",
       "    \"ParseDelimited\",\n",
       "    \"DropColumns\",\n",
       "    \"SetColumnTypes\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1611832348334
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\" :30,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'average_precision_score_weighted',\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(\n",
    "    experiment_timeout_minutes=30,\n",
    "    n_cross_validations=3,\n",
    "    task=\"classification\",\n",
    "    primary_metric=\"average_precision_score_weighted\",\n",
    "    compute_target=cpu_cluster,\n",
    "    training_data=training_data,\n",
    "    label_column_name=\"PCOS\",\n",
    "    max_cores_per_iteration=-1,\n",
    "    enable_onnx_compatible_models=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Submitting the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1611834549346
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote.\n",
      "No run_configuration provided, running on cpucluster-aml with default configuration\n",
      "Running on remote compute: cpucluster-aml\n",
      "Parent Run ID: AutoML_2753db88-bcd0-4293-a90c-891b78b4837f\n",
      "\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturization. Beginning to fit featurizers and featurize the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Class balancing detection\n",
      "STATUS:       ALERTED\n",
      "DESCRIPTION:  To decrease model bias, please cancel the current run and fix balancing problem.\n",
      "              Learn more about imbalanced data: https://aka.ms/AutomatedMLImbalancedData\n",
      "DETAILS:      Imbalanced data can lead to a falsely perceived positive effect of a model's accuracy because the input data has bias towards one class.\n",
      "+---------------------------------+---------------------------------+--------------------------------------+\n",
      "|Size of the smallest class       |Name/Label of the smallest class |Number of samples in the training data|\n",
      "+=================================+=================================+======================================+\n",
      "|1                                | mb n                            |96                                    |\n",
      "+---------------------------------+---------------------------------+--------------------------------------+\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "TYPE:         Missing feature values imputation\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  No feature missing values were detected in the training data.\n",
      "              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       DONE\n",
      "DESCRIPTION:  High cardinality features were detected in your inputs and handled.\n",
      "              Learn more about high cardinality feature handling: https://aka.ms/AutomatedMLFeaturization\n",
      "DETAILS:      High cardinality features refer to columns that contain a large percentage of unique values.\n",
      "+---------------------------------+---------------------------------+\n",
      "|Column name                      |Column Content Type              |\n",
      "+=================================+=================================+\n",
      "|regular_periods                  |categorical_hash                 |\n",
      "|sleep_time                       |categorical_hash                 |\n",
      "|wake_time                        |categorical_hash                 |\n",
      "|fast_food_frequency              |categorical_hash                 |\n",
      "+---------------------------------+---------------------------------+\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   MaxAbsScaler LightGBM                          0:00:43       0.9115    0.9115\n",
      "         1   MaxAbsScaler XGBoostClassifier                 0:00:56       0.9775    0.9775\n",
      "         2   MaxAbsScaler ExtremeRandomTrees                0:00:43       0.9643    0.9775\n",
      "         3   MaxAbsScaler ExtremeRandomTrees                0:00:49       0.9650    0.9775\n",
      "         4   MaxAbsScaler ExtremeRandomTrees                0:00:49       0.9609    0.9775\n",
      "         5   MaxAbsScaler RandomForest                      0:00:55       0.9524    0.9775\n",
      "         6   MaxAbsScaler ExtremeRandomTrees                0:00:45       0.9705    0.9775\n",
      "         7   MaxAbsScaler RandomForest                      0:00:50       0.9467    0.9775\n",
      "         8   MaxAbsScaler RandomForest                      0:00:46       0.9662    0.9775\n",
      "         9   StandardScalerWrapper XGBoostClassifier        0:00:49       0.9623    0.9775\n",
      "        10   MaxAbsScaler ExtremeRandomTrees                0:00:45       0.9571    0.9775\n",
      "        11   MaxAbsScaler RandomForest                      0:00:43       0.9333    0.9775\n",
      "        12   MaxAbsScaler RandomForest                      0:00:43       0.9553    0.9775\n",
      "        13   MaxAbsScaler ExtremeRandomTrees                0:00:50       0.9597    0.9775\n",
      "        14   SparseNormalizer ExtremeRandomTrees            0:00:55       0.9724    0.9775\n",
      "        15   MaxAbsScaler ExtremeRandomTrees                0:00:45       0.9639    0.9775\n",
      "        16   TruncatedSVDWrapper XGBoostClassifier          0:00:45       0.9354    0.9775\n",
      "        17   SparseNormalizer XGBoostClassifier             0:00:46       0.9516    0.9775\n",
      "        18   SparseNormalizer LightGBM                      0:00:46       0.9692    0.9775\n",
      "        19   MaxAbsScaler ExtremeRandomTrees                0:01:05       0.9655    0.9775\n",
      "        20   StandardScalerWrapper LogisticRegression       0:00:46       0.9585    0.9775\n",
      "        21   SparseNormalizer LightGBM                      0:00:45       0.9651    0.9775\n",
      "        22   SparseNormalizer XGBoostClassifier             0:00:47       0.9564    0.9775\n",
      "        23   MaxAbsScaler ExtremeRandomTrees                0:00:52       0.9646    0.9775\n",
      "        24   StandardScalerWrapper LogisticRegression       0:00:43       0.9706    0.9775\n",
      "        25   SparseNormalizer XGBoostClassifier             0:00:46       0.9098    0.9775\n",
      "        26   MaxAbsScaler LogisticRegression                0:00:41       0.9597    0.9775\n",
      "        27   StandardScalerWrapper ExtremeRandomTrees       0:00:55       0.9398    0.9775\n",
      "        28   MaxAbsScaler LightGBM                          0:00:45       0.9494    0.9775\n",
      "        29    VotingEnsemble                                0:01:16       0.9816    0.9816\n"
     ]
    }
   ],
   "source": [
    "remote_run = experiment.submit(config = automl_config, show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "The `Rundetails` widget, as the name suggests gives us greater insight about how the Run is proceeding, enabling us to monitor and understand the situation, thereby dealing with it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1611834549686
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "The best performing model is the `VotingEnsemble` with a score of 0.9006. It maybe observed to be derived from the following:\n",
    "\n",
    "|**Field**|Value|\n",
    "|-|-|\n",
    "|**Ensembled Iterations**|0, 14, 15, 6, 26|\n",
    "|**Ensembled Algorithms**|'LightGBM', 'RandomForest', 'XGBoostClassifier', 'ExtremeRandomTrees', 'XGBoostClassifier'|\n",
    "|**Ensemble Weights**|0.3333333333333333, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666|\n",
    "|**Best Individual Pipeline Score**|\"0.9005922928114609\"|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gather": {
     "logged": 1611837585537
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: pcos_automl,\n",
      "Id: AutoML_2753db88-bcd0-4293-a90c-891b78b4837f_29,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('datatransformer',\n",
      "                 DataTransformer(enable_dnn=None, enable_feature_sweeping=None,\n",
      "                                 feature_sweeping_config=None,\n",
      "                                 feature_sweeping_timeout=None,\n",
      "                                 featurization_config=None, force_text_dnn=None,\n",
      "                                 is_cross_validation=None,\n",
      "                                 is_onnx_compatible=None, logger=None,\n",
      "                                 observer=None, task=None, working_dir=None)),\n",
      "                ('prefittedsoftvotingclassifier',...\n",
      "                                                                                                objective=None,\n",
      "                                                                                                random_state=None,\n",
      "                                                                                                reg_alpha=0.0,\n",
      "                                                                                                reg_lambda=0.0,\n",
      "                                                                                                silent=True,\n",
      "                                                                                                subsample=1.0,\n",
      "                                                                                                subsample_for_bin=200000,\n",
      "                                                                                                subsample_freq=0,\n",
      "                                                                                                verbose=-10))],\n",
      "                                                                     verbose=False))],\n",
      "                                               flatten_transform=None,\n",
      "                                               weights=[0.1111111111111111,\n",
      "                                                        0.1111111111111111,\n",
      "                                                        0.1111111111111111,\n",
      "                                                        0.1111111111111111,\n",
      "                                                        0.1111111111111111,\n",
      "                                                        0.2222222222222222,\n",
      "                                                        0.1111111111111111,\n",
      "                                                        0.1111111111111111]))],\n",
      "         verbose=False)\n",
      "Y_transformer(['LabelEncoder', LabelEncoder()])\n"
     ]
    }
   ],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)\n",
    "\n",
    "model_ml = best_run.register_model(model_name='PCOS_auto_ml', model_path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Retrieve and Save ONNX Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611834703886
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
    "\n",
    "b_run , onnx_mdl = remote_run.get_output(return_onnx_model=True)\n",
    "onnx_fl_path = \"./best_model.onnx\"\n",
    "OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Predict with the ONNX model, using onnxruntime package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611837571044
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mb n' 'mb n' 'mb n' 'mb' 'mb' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n' 'mb n'\n",
      " 'mb n' 'mb n' 'mb n' 'mb n' 'mb n']\n",
      "[[0.01430098 0.13477242 0.85092664]\n",
      " [0.02792714 0.1486565  0.8234164 ]\n",
      " [0.02050737 0.13300157 0.84649116]\n",
      " [0.10077284 0.70155394 0.19767326]\n",
      " [0.03942947 0.4998218  0.4607487 ]\n",
      " [0.0199182  0.13074069 0.8493412 ]\n",
      " [0.03091824 0.14138481 0.82769704]\n",
      " [0.04183062 0.20063189 0.7575376 ]\n",
      " [0.03044434 0.11633296 0.8532228 ]\n",
      " [0.03955894 0.16426148 0.79617965]\n",
      " [0.02454297 0.1684966  0.80696046]\n",
      " [0.03239671 0.17921084 0.78839254]\n",
      " [0.03904817 0.12187015 0.83908176]\n",
      " [0.03949186 0.21823603 0.74227214]\n",
      " [0.08617711 0.14261936 0.7712036 ]\n",
      " [0.02691026 0.13311693 0.83997285]\n",
      " [0.01398168 0.1435816  0.8424368 ]\n",
      " [0.01328343 0.12634854 0.86036813]\n",
      " [0.01725763 0.12192886 0.8608136 ]\n",
      " [0.04178904 0.16657779 0.79163325]\n",
      " [0.05259122 0.13467537 0.8127335 ]\n",
      " [0.05053617 0.25219345 0.69727045]\n",
      " [0.05259122 0.13467537 0.8127335 ]\n",
      " [0.01282299 0.15287656 0.8343005 ]\n",
      " [0.06417013 0.18453032 0.7512996 ]\n",
      " [0.0143319  0.1281142  0.857554  ]\n",
      " [0.0143319  0.1281142  0.857554  ]\n",
      " [0.15793264 0.17562717 0.66644025]\n",
      " [0.02505583 0.13121529 0.84372896]\n",
      " [0.0291407  0.16692051 0.80393887]\n",
      " [0.02910375 0.13668449 0.8342118 ]\n",
      " [0.03044434 0.11633296 0.8532228 ]\n",
      " [0.02503905 0.22919564 0.7457654 ]\n",
      " [0.01246197 0.12761046 0.85992765]\n",
      " [0.01687425 0.14329931 0.83982646]\n",
      " [0.01380414 0.20962448 0.7765714 ]\n",
      " [0.04293741 0.1272689  0.82979375]\n",
      " [0.04064883 0.31093606 0.64841515]\n",
      " [0.00877222 0.12423074 0.8669971 ]\n",
      " [0.12143143 0.15712579 0.7214428 ]\n",
      " [0.08602095 0.18305618 0.73092294]\n",
      " [0.03000405 0.11619598 0.85380006]\n",
      " [0.02910375 0.13668449 0.8342118 ]\n",
      " [0.02565548 0.13843924 0.8359054 ]\n",
      " [0.02044796 0.18150555 0.7980465 ]\n",
      " [0.07159647 0.33521894 0.59318465]\n",
      " [0.01328343 0.12634854 0.86036813]\n",
      " [0.03984058 0.17057571 0.7895838 ]\n",
      " [0.09205011 0.14684027 0.7611097 ]\n",
      " [0.04084094 0.12208618 0.83707297]\n",
      " [0.00891027 0.12319591 0.8678939 ]\n",
      " [0.01424112 0.16354568 0.8222133 ]\n",
      " [0.02691026 0.13311693 0.83997285]\n",
      " [0.04863971 0.17985013 0.77151024]\n",
      " [0.04355077 0.1455133  0.810936  ]\n",
      " [0.04889628 0.15458238 0.7965214 ]\n",
      " [0.034208   0.17946684 0.7863252 ]\n",
      " [0.06417013 0.18453032 0.7512996 ]\n",
      " [0.03949186 0.21823603 0.74227214]\n",
      " [0.05078491 0.11966449 0.8295507 ]\n",
      " [0.05166242 0.17292398 0.77541363]\n",
      " [0.04667839 0.13710567 0.816216  ]\n",
      " [0.02696114 0.25521532 0.7178236 ]\n",
      " [0.02192673 0.13355729 0.84451604]\n",
      " [0.03635472 0.12066806 0.8429773 ]\n",
      " [0.01380414 0.20962448 0.7765714 ]\n",
      " [0.06878082 0.1853959  0.7458234 ]\n",
      " [0.01842585 0.12487398 0.85670024]\n",
      " [0.00912644 0.12338112 0.8674925 ]\n",
      " [0.04003952 0.14288978 0.8170707 ]\n",
      " [0.05053617 0.25219345 0.69727045]\n",
      " [0.03922059 0.29518145 0.66559803]\n",
      " [0.0228309  0.13252825 0.84464085]\n",
      " [0.08761834 0.1334166  0.7789651 ]\n",
      " [0.12143143 0.15712579 0.7214428 ]\n",
      " [0.03696362 0.15682529 0.8062111 ]\n",
      " [0.03311925 0.16506168 0.8018191 ]\n",
      " [0.04318284 0.12987421 0.82694304]\n",
      " [0.01736096 0.13954176 0.8430973 ]\n",
      " [0.03030187 0.164343   0.8053552 ]\n",
      " [0.01965769 0.13525268 0.8450897 ]\n",
      " [0.03974472 0.53933704 0.4209183 ]\n",
      " [0.07159647 0.33521894 0.59318465]\n",
      " [0.01663499 0.13747044 0.8458947 ]\n",
      " [0.04183062 0.20063189 0.7575376 ]\n",
      " [0.04892666 0.14006965 0.81100374]\n",
      " [0.04355077 0.1455133  0.810936  ]\n",
      " [0.03397721 0.17559586 0.79042697]\n",
      " [0.04003983 0.14289615 0.8170641 ]\n",
      " [0.07191761 0.13039196 0.7976905 ]\n",
      " [0.00858844 0.16875483 0.82265687]\n",
      " [0.03673038 0.13518333 0.8280864 ]\n",
      " [0.02185861 0.13900878 0.83913267]\n",
      " [0.01478246 0.16248351 0.8227341 ]\n",
      " [0.04759714 0.32465667 0.6277462 ]\n",
      " [0.03922059 0.29518145 0.66559803]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from azureml.automl.core.onnx_convert import OnnxConvertConstants\n",
    "from azureml.train.automl import constants\n",
    "\n",
    "if sys.version_info < OnnxConvertConstants.OnnxIncompatiblePythonVersion:\n",
    "    python_version_compatible = True\n",
    "else:\n",
    "    python_version_compatible = False\n",
    "\n",
    "import onnxruntime\n",
    "from azureml.automl.runtime.onnx_convert import OnnxInferenceHelper\n",
    "\n",
    "def get_onnx_res(run):\n",
    "    res_path = 'onnx_resource.json'\n",
    "    run.download_file(name=constants.MODEL_RESOURCE_PATH_ONNX, output_file_path=res_path)\n",
    "    with open(res_path) as f:\n",
    "        onnx_res = json.load(f)\n",
    "    return onnx_res\n",
    "\n",
    "if python_version_compatible:\n",
    "    mdl_bytes = onnx_mdl.SerializeToString()\n",
    "    onnx_res = get_onnx_res(b_run)\n",
    "\n",
    "    df_test['Column1_1'] = 0.0\n",
    "    onnxrt_helper = OnnxInferenceHelper(mdl_bytes, onnx_res)\n",
    "    pred_onnx, pred_prob_onnx = onnxrt_helper.predict(df_test)\n",
    "\n",
    "    print(pred_onnx)\n",
    "    print(pred_prob_onnx)\n",
    "else:\n",
    "    print('Please use Python version 3.6 or 3.7 to run the inference helper.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Being the better performing model, I shall hereby deploy the `VotingEnsemble` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611395008698
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1611395011705
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./amlmodel', exist_ok=True)\n",
    "\n",
    "best_run.download_file('/outputs/model.pkl',os.path.join('./amlmodel','automl_best_model_cc.pkl'))\n",
    "\n",
    "for f in best_run.get_file_names():\n",
    "    if f.startswith('outputs'):\n",
    "        output_file_path = os.path.join('./amlmodel', f.split('/')[-1])\n",
    "        print(f'Downloading from {f} to {output_file_path} ...')\n",
    "        best_run.download_file(name=f, output_file_path=output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1611395015772
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model=best_run.register_model(\n",
    "            model_name = 'automl-bestmodel-cc', \n",
    "            model_path = './outputs/model.pkl',\n",
    "            model_framework=Model.Framework.SCIKITLEARN,\n",
    "            description='Cervical Cancer Prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611395016084
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Download the conda environment file and define the environement\n",
    "best_run.download_file('outputs/conda_env_v_1_0_0.yml', 'conda_env.yml')\n",
    "myenv = Environment.from_conda_specification(name = 'myenv',\n",
    "                                             file_path = 'conda_env.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1611395017072
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# download the scoring file produced by AutoML\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', 'score_auto_cc.py')\n",
    "\n",
    "# set inference config\n",
    "inference_config = InferenceConfig(entry_script= 'score_auto_cc.py',\n",
    "                                    environment=myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611395018470
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# set Aci Webservice config\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611395025920
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "service = Model.deploy(workspace=ws, \n",
    "                       name='automl-bestmodel-cc', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611395026107
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611396043840
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# wait for deployment to finish and display the scoring uri and swagger uri\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print('Service state:')\n",
    "print(service.state)\n",
    "\n",
    "print('Scoring URI:')\n",
    "print(service.scoring_uri)\n",
    "\n",
    "print('Swagger URI:')\n",
    "print(service.swagger_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611396043903
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# select 3  samples from the dataframe\n",
    "x_df=df.sample(3)\n",
    "y_df = x_df.pop('PCOS')\n",
    "\n",
    "x_df['Column1'] = 0.0\n",
    "\n",
    "# convert the records to a json data file\n",
    "recored=x_df.to_dict(orient='records')\n",
    "\n",
    "scoring_json = json.dumps({'data': recored})\n",
    "print(scoring_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Consumint the endpoint using `endpoint.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!python3 endpoint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611396055125
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "output = service.run(scoring_json)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1611398172008
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Enabling logging using `logs.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!python3 logs.py"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
